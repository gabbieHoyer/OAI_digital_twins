{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22937,
     "status": "ok",
     "timestamp": 1702337944875,
     "user": {
      "displayName": "Gabrielle Hoyer",
      "userId": "08258312017408625190"
     },
     "user_tz": 480
    },
    "id": "ylb_9bIZa9Bh",
    "outputId": "b5f604b5-8006-4801-ade9-5e1cd7360863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QI6oTGKMFiyG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_rel, false_discovery_control\n",
    "from scipy.stats import shapiro, anderson, wilcoxon, chi2_contingency, contingency, pointbiserialr\n",
    "\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18231,
     "status": "ok",
     "timestamp": 1702338016809,
     "user": {
      "displayName": "Gabrielle Hoyer",
      "userId": "08258312017408625190"
     },
     "user_tz": 480
    },
    "id": "cOZOsr5jFuxz",
    "outputId": "96b8d44d-120c-42fc-a26c-b4532cac56a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Colab_Notebooks/oai/TKR_twin\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd '/content/drive/MyDrive/Colab_Notebooks/oai/TKR_twin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHoa30C3Fu25"
   },
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKBx9GG8FzSV"
   },
   "source": [
    "**Combine Matched Subject IDs with PC modes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEeXw85eFu5z"
   },
   "outputs": [],
   "source": [
    "def load_and_merge_data(full_data_path, match_data_path, merge_key, selected_columns):\n",
    "    full_df = pd.read_csv(full_data_path)\n",
    "    match_df = pd.read_csv(match_data_path, usecols=selected_columns)\n",
    "    merged_df = pd.merge(match_df, full_df, on=merge_key, how='left')\n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "selected_columns = ['new_id', 'subclass', 'weights', 'distance', 'id']\n",
    "oa_inc_matched_df = load_and_merge_data(\n",
    "    'publish_dataframes/oa_inc_multiple_imputation_filled.csv',\n",
    "    'publish_dataframes/oa_inc_matchit_nearestNeightborMethod_noReplacement_df.csv',\n",
    "    'id',\n",
    "    selected_columns\n",
    ")\n",
    "\n",
    "tkr_matched_df = load_and_merge_data(\n",
    "    'publish_dataframes/tkr_multiple_imputation_filled.csv',\n",
    "    'publish_dataframes/tkr_matchit_nearestNeightborMethod_noReplacement_df.csv',\n",
    "    'id',\n",
    "    selected_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghx27XOOF4UC"
   },
   "source": [
    "**Output Dataframes**:\n",
    "\n",
    "*oa_inc_matched_df*: 'publish_dataframes/oa_inc_matched_IDs_PC_modes.csv'\n",
    "\n",
    "*tkr_matched_df*: 'publish_dataframes/tkr_matched_IDs_PC_modes.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0woNEt_wGDK2"
   },
   "source": [
    "### **Evaluating Matching Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA40GpT_F9oC"
   },
   "source": [
    "**Normality and Homoscedasticity Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFfTV3i4Fu8S"
   },
   "outputs": [],
   "source": [
    "def perform_normality_tests(dataframe, group_column, column_range):\n",
    "    # Extracting groups\n",
    "    group_control = dataframe[dataframe[group_column] == 0].reset_index(drop=True)\n",
    "    group_treatment = dataframe[dataframe[group_column] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Ensure equal sizes\n",
    "    assert len(group_control) == len(group_treatment), \"Groups are not paired correctly!\"\n",
    "\n",
    "    # Define a DataFrame to hold our results\n",
    "    results = pd.DataFrame(columns=['variable', 'shapiro_stat', 'shapiro_p', 'anderson_stat', 'anderson_critical_values', 'anderson_significance_level'])\n",
    "\n",
    "    # Determine the slicing range\n",
    "    if isinstance(column_range, tuple):\n",
    "        start, end = column_range\n",
    "        selected_columns = dataframe.iloc[:, start:end if end is not None else None].columns\n",
    "    elif isinstance(column_range, slice):\n",
    "        selected_columns = dataframe.iloc[:, column_range].columns\n",
    "    else:\n",
    "        raise ValueError(\"column_range must be a slice or a tuple\")\n",
    "\n",
    "    # Loop through all variable columns\n",
    "    for col in selected_columns:\n",
    "        differences = group_treatment[col] - group_control[col]\n",
    "\n",
    "        # Drop NA values from differences\n",
    "        differences = differences.dropna()\n",
    "\n",
    "        # Shapiro-Wilk Test\n",
    "        shapiro_stat, shapiro_p = shapiro(differences)\n",
    "\n",
    "        # Anderson-Darling Test\n",
    "        anderson_result = anderson(differences)\n",
    "        anderson_stat = anderson_result.statistic\n",
    "        anderson_critical_values = anderson_result.critical_values\n",
    "        anderson_significance_level = anderson_result.significance_level\n",
    "\n",
    "        # Append results\n",
    "        results = results.append({\n",
    "            'variable': col,\n",
    "            'shapiro_stat': shapiro_stat,\n",
    "            'shapiro_p': shapiro_p,\n",
    "            'anderson_stat': anderson_stat,\n",
    "            'anderson_critical_values': anderson_critical_values,\n",
    "            'anderson_significance_level': anderson_significance_level\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwjERUH_GJ3E"
   },
   "source": [
    "Checking Normality and Homoscedasticity of Twin/Matched Subject continuous numerical clinical factors and demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23zgCoO9Fu-8"
   },
   "outputs": [],
   "source": [
    "oa_inc_numerical_results = perform_normality_tests(oa_inc_matched_df, 'oa_prog', (20, -110))\n",
    "\n",
    "tkr_numerical_results = perform_normality_tests(tkr_matched_df, 'tkr', (20, -110))\n",
    "\n",
    "# Optionally, save the results to Excel files\n",
    "oa_inc_numerical_results.to_excel('publish_dataframes/OA_Inc_demos_clinicalFactors_normality_tests_results.xlsx', index=False, engine='openpyxl')\n",
    "tkr_numerical_results.to_excel('publish_dataframes/TKR_demos_clinicalFactors_normality_tests_results.xlsx', index=False, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEOR13qnGNzg"
   },
   "source": [
    "**Output Statistics**:\n",
    "\n",
    "*oa_inc_numerical_results*:\n",
    "\n",
    "'publish_dataframes/OA_Inc_demos_clinicalFactors_normality_tests_results.xlsx'\n",
    "\n",
    "*tkr_numerical_results*:\n",
    "\n",
    " 'publish_dataframes/TKR_demos_clinicalFactors_normality_tests_results.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVuxZviLWcNK"
   },
   "source": [
    "**Descriptive Statistics for Twin Cohorts Before and After Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkjnJt0HFvBT"
   },
   "outputs": [],
   "source": [
    "def calculate_group_stats(group_data, numerical_cols, categorical_cols):\n",
    "    # Numerical Descriptive Statistics\n",
    "    numerical_desc = group_data[numerical_cols].describe()\n",
    "    numerical_iqr = group_data[numerical_cols].apply(lambda x: x.quantile(0.75) - x.quantile(0.25))\n",
    "    numerical_iqr = pd.DataFrame(numerical_iqr, columns=['IQR'])\n",
    "    numerical_desc.loc['median'] = numerical_desc.loc['50%']\n",
    "    numerical_desc.loc['IQR'] = numerical_iqr['IQR']\n",
    "\n",
    "    # Categorical Descriptive Statistics\n",
    "    combined_categorical_stats = pd.DataFrame(columns=['Variable', 'Category', 'N', '%'])\n",
    "    for cat_var in categorical_cols:\n",
    "        counts = group_data[cat_var].value_counts(normalize=False)\n",
    "        percents = group_data[cat_var].value_counts(normalize=True) * 100\n",
    "        temp_df = pd.DataFrame({\n",
    "            'Variable': [cat_var] * len(counts),\n",
    "            'Category': counts.index,\n",
    "            'N': counts.values,\n",
    "            '%': percents.values,\n",
    "        })\n",
    "        combined_categorical_stats = pd.concat([combined_categorical_stats, temp_df], ignore_index=True)\n",
    "\n",
    "    return numerical_desc.loc[['count', 'mean', 'median', 'std', 'IQR']], combined_categorical_stats\n",
    "\n",
    "def calculate_and_save_group_stats(dataframe, group_column, group_names, numerical_cols, categorical_cols, file_name):\n",
    "    with pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n",
    "        # Extracting unique values to define groups\n",
    "        unique_groups = dataframe[group_column].unique()\n",
    "\n",
    "        for group in unique_groups:\n",
    "            subset = dataframe[dataframe[group_column] == group]\n",
    "\n",
    "            numerical_stats, categorical_stats = calculate_group_stats(subset, numerical_cols, categorical_cols)\n",
    "\n",
    "            # Use custom group name if provided, else use the group value as the name\n",
    "            group_name = group_names.get(group, str(group))\n",
    "\n",
    "            # Writing numerical and categorical statistics to Excel\n",
    "            numerical_stats.to_excel(writer, sheet_name=f\"{group_name}_Numerical\")\n",
    "            categorical_stats.to_excel(writer, sheet_name=f\"{group_name}_Categorical\")\n",
    "\n",
    "        # Adding Overall Statistics\n",
    "        numerical_stats, categorical_stats = calculate_group_stats(dataframe, numerical_cols, categorical_cols)\n",
    "        numerical_stats.to_excel(writer, sheet_name=\"Overall_Numerical\")\n",
    "        categorical_stats.to_excel(writer, sheet_name=\"Overall_Categorical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQq-5Opodk0f"
   },
   "source": [
    "**Before Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H341BNWEWsC-"
   },
   "outputs": [],
   "source": [
    "# OA Incidence example usage\n",
    "oa_inc_before_match = pd.read_csv('publish_dataframes/oa_inc_multiple_imputation_filled.csv')\n",
    "numerical_cols = oa_inc_before_match.iloc[:,16:-110].columns\n",
    "categorical_cols = oa_inc_before_match.iloc[:,7:16].columns\n",
    "group_names = {0: 'Control', 1: 'OA_Inc Group'}\n",
    "\n",
    "calculate_and_save_group_stats(oa_inc_before_match, 'oa_prog', group_names, numerical_cols, categorical_cols, \"publish_dataframes/OA_Inc_before_matching_descriptive_statistics_output.xlsx\")\n",
    "\n",
    "# TKR example usage\n",
    "tkr_before_match = pd.read_csv('publish_dataframes/tkr_multiple_imputation_filled.csv')\n",
    "numerical_cols = tkr_before_match.iloc[:,16:-110].columns\n",
    "categorical_cols = tkr_before_match.iloc[:,7:16].columns\n",
    "group_names = {0: 'Control', 1: 'TKR Group'}\n",
    "\n",
    "calculate_and_save_group_stats(tkr_before_match, 'tkr', group_names, numerical_cols, categorical_cols, \"publish_dataframes/TKR_before_matching_descriptive_statistics_output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QnhjLH0d4Xk"
   },
   "source": [
    "**Output Statistics**:\n",
    "\n",
    "*OA Incidence*: \"publish_dataframes/OA_Inc_before_matching_descriptive_statistics_output.xlsx\"\n",
    "\n",
    "\n",
    "*TKR*: \"publish_dataframes/TKR_before_matching_descriptive_statistics_output.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gricghlKdoom"
   },
   "source": [
    "**After Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1g38LRsIWsFf"
   },
   "outputs": [],
   "source": [
    "# OA Incidence example usage\n",
    "numerical_cols = oa_inc_matched_df.iloc[:,20:-110].columns\n",
    "categorical_cols = oa_inc_matched_df.iloc[:,11:20].columns\n",
    "group_names = {0: 'Control', 1: 'OA_Inc Group'}\n",
    "\n",
    "calculate_and_save_group_stats(oa_inc_matched_df, 'oa_prog', group_names, numerical_cols, categorical_cols, \"publish_dataframes/OA_Inc_Twins_descriptive_statistics_output.xlsx\")\n",
    "\n",
    "# TKR example usage\n",
    "numerical_cols = tkr_matched_df.iloc[:,20:-110].columns\n",
    "categorical_cols = tkr_matched_df.iloc[:,11:20].columns\n",
    "group_names = {0: 'Control', 1: 'TKR Group'}\n",
    "\n",
    "calculate_and_save_group_stats(tkr_matched_df, 'tkr', group_names, numerical_cols, categorical_cols, \"publish_dataframes/TKR_Twins_descriptive_statistics_output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TicMyCoZdsGq"
   },
   "source": [
    "**Output Statistics**:\n",
    "\n",
    "*OA Incidence*:\n",
    "\"publish_dataframes/OA_Inc_Twins_descriptive_statistics_output.xlsx\"\n",
    "\n",
    "*TKR*: \"publish_dataframes/TKR_Twins_descriptive_statistics_output.xlsx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxL4WXQ_sxBw"
   },
   "source": [
    "**Effect Size Statistics - point biserial correlation coefficient, SMD, and Cramer's V**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLuzRd0FF3PN"
   },
   "outputs": [],
   "source": [
    "def calculate_effect_size_stats_and_save(dataframe, target_column, numerical_cols, categorical_cols, file_name):\n",
    "    # Extracting groups\n",
    "    group_control = dataframe[dataframe[target_column] == 0].reset_index(drop=True)\n",
    "    group_treatment = dataframe[dataframe[target_column] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Define named tuples for results\n",
    "    NumericalTestResults = namedtuple('NumericalTestResults', ['column', 'SMD', 'biserial_corr', 'p_value', 'test_type'])\n",
    "    CategoricalTestResults = namedtuple('CategoricalTestResults', ['column', 'cramers_v', 'test_type'])\n",
    "\n",
    "    numerical_results = []\n",
    "    categorical_results = []\n",
    "\n",
    "    # Calculate metrics for numerical columns\n",
    "    for col in numerical_cols:\n",
    "        # Calculate SMD\n",
    "        control_mean = group_control[col].mean()\n",
    "        treatment_mean = group_treatment[col].mean()\n",
    "        pooled_std = np.sqrt((group_control[col].std()**2 + group_treatment[col].std()**2) / 2)\n",
    "        smd = (treatment_mean - control_mean) / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "        # Calculate Point Biserial Correlation Coefficient\n",
    "        y = dataframe[col].dropna()\n",
    "        x = dataframe.loc[y.index, target_column]\n",
    "        biserial_corr, biserial_p_value = pointbiserialr(x, y)\n",
    "\n",
    "        # Append results\n",
    "        numerical_results.append(NumericalTestResults(col, smd, biserial_corr, biserial_p_value, 'numerical'))\n",
    "\n",
    "    # Calculate metrics for categorical columns\n",
    "    for col in categorical_cols:\n",
    "        # Calculate Cramer's V\n",
    "        contingency_table = pd.crosstab(dataframe[target_column], dataframe[col])\n",
    "        # Replace the following line with your actual function call to calculate Cramer's V\n",
    "        cramers_v = contingency.association(contingency_table, method='cramer', correction=False)\n",
    "\n",
    "        # Append results\n",
    "        categorical_results.append(CategoricalTestResults(col, cramers_v, 'cramers_v'))\n",
    "\n",
    "    # Converting results into DataFrames\n",
    "    numerical_results_df = pd.DataFrame(numerical_results)\n",
    "    categorical_results_df = pd.DataFrame(categorical_results)\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    with pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n",
    "        numerical_results_df.to_excel(writer, sheet_name='Numerical_Statistics')\n",
    "        categorical_results_df.to_excel(writer, sheet_name='Categorical_Statistics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjhwTjlZXvdM"
   },
   "source": [
    "**Before Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiTpsqxgkfJX"
   },
   "outputs": [],
   "source": [
    "# Before Matching\n",
    "# OA Incidence example usage\n",
    "oa_inc_before_match = pd.read_csv('publish_dataframes/oa_inc_multiple_imputation_filled.csv')\n",
    "numerical_cols = oa_inc_before_match.iloc[:,16:-110].columns.tolist()\n",
    "categorical_cols = oa_inc_before_match.iloc[:,7:16].columns.tolist()\n",
    "calculate_effect_size_stats_and_save(oa_inc_before_match, 'oa_prog', numerical_cols, categorical_cols, \"publish_dataframes/OA_Inc_Twins_before_matching_effectSize_statistics_output.xlsx\")\n",
    "\n",
    "# TKR example usage\n",
    "tkr_before_match = pd.read_csv('publish_dataframes/tkr_multiple_imputation_filled.csv')\n",
    "numerical_cols = tkr_before_match.iloc[:,16:-110].columns.tolist()\n",
    "categorical_cols = tkr_before_match.iloc[:,7:16].columns.tolist()\n",
    "calculate_effect_size_stats_and_save(tkr_before_match, 'tkr', numerical_cols, categorical_cols, \"publish_dataframes/TKR_Twins_before_matching_effectSize_statistics_output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTcGbNgzWEFs"
   },
   "source": [
    "**Output Stats**:\n",
    "\n",
    "*OA Incidence*: \"publish_dataframes/OA_Inc_Twins_before_matching_effectSize_statistics_output.xlsx\"\n",
    "\n",
    "*TKR*: \"publish_dataframes/TKR_Twins_before_matching_effectSize_statistics_output.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "un1d6kBGXx03"
   },
   "source": [
    "**After Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1zJlNcvXx-4"
   },
   "outputs": [],
   "source": [
    "# After Matching\n",
    "# OA Incidence example usage\n",
    "numerical_cols = oa_inc_matched_df.iloc[:,20:-110].columns.tolist()\n",
    "categorical_cols = oa_inc_matched_df.iloc[:,11:20].columns.tolist()\n",
    "calculate_effect_size_stats_and_save(oa_inc_matched_df, 'oa_prog', numerical_cols, categorical_cols, \"publish_dataframes/OA_Inc_Twins_effectSize_statistics_output.xlsx\")\n",
    "\n",
    "# TKR example usage\n",
    "numerical_cols = tkr_matched_df.iloc[:,20:-110].columns.tolist()\n",
    "categorical_cols = tkr_matched_df.iloc[:,11:20].columns.tolist()\n",
    "calculate_effect_size_stats_and_save(tkr_matched_df, 'tkr', numerical_cols, categorical_cols, \"publish_dataframes/TKR_Twins_effectSize_statistics_output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hHiL264XyJQ"
   },
   "source": [
    "**Output Stats**:\n",
    "\n",
    "*OA Incidence*: \"publish_dataframes/OA_Inc_Twins_effectSize_statistics_output.xlsx\"\n",
    "\n",
    "*TKR*: \"publish_dataframes/TKR_Twins_effectSize_statistics_output.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JI-2aZWF4Af"
   },
   "source": [
    "**Non-Parametric Statistical Hypothesis Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ym0gyvLhFvEN"
   },
   "outputs": [],
   "source": [
    "def bootstrap_median_difference(treatment_col, control_col, n_bootstrap=1000, confidence_levels=[95, 99], seed=None):\n",
    "    \"\"\"\n",
    "    Performs bootstrap analysis to estimate confidence intervals of the median differences between two columns.\n",
    "\n",
    "    Parameters:\n",
    "    treatment_col (array-like): The treatment group column.\n",
    "    control_col (array-like): The control group column.\n",
    "    n_bootstrap (int): The number of bootstrap samples to draw.\n",
    "    confidence_levels (list): The confidence levels for which to compute the intervals.\n",
    "    seed (int, optional): The seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    dict: Confidence intervals for each specified level.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if len(treatment_col) != len(control_col):\n",
    "        raise ValueError(\"Treatment and control columns must be of the same length.\")\n",
    "\n",
    "    bootstrapped_medians = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        resampled_oa = np.random.choice(treatment_col, size=len(treatment_col), replace=True)\n",
    "        resampled_control = np.random.choice(control_col, size=len(control_col), replace=True)\n",
    "        median_diff = np.median(resampled_oa - resampled_control)\n",
    "        bootstrapped_medians.append(median_diff)\n",
    "\n",
    "    ci_bounds = {}\n",
    "    for level in confidence_levels:\n",
    "        if not (0 < level < 100):\n",
    "            raise ValueError(\"Confidence levels must be between 0 and 100.\")\n",
    "        lower_bound = np.percentile(bootstrapped_medians, (100 - level) / 2)\n",
    "        upper_bound = np.percentile(bootstrapped_medians, 100 - (100 - level) / 2)\n",
    "        ci_bounds[level] = (lower_bound, upper_bound)\n",
    "\n",
    "    return ci_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAZyY-udsz6e"
   },
   "outputs": [],
   "source": [
    "def calculate_nonparametric_stats_and_save(dataframe, target_column, numerical_cols, categorical_cols, file_name):\n",
    "    # Extracting groups\n",
    "    group_control = dataframe[dataframe[target_column] == 0].reset_index(drop=True)\n",
    "    group_treatment = dataframe[dataframe[target_column] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Ensure equal sizes\n",
    "    assert len(group_control) == len(group_treatment), \"Groups are not paired correctly!\"\n",
    "\n",
    "    # Define named tuples for results\n",
    "    WilcoxonTestResults = namedtuple('WilcoxonTestResults', ['column', 'wilcoxon_stat', 'p_value', 'dof', 'ci_95_low', 'ci_95_high', 'ci_99_low', 'ci_99_high', 'test_type', 'median_diff', 'margin_of_err95', 'margin_of_err99', 'point_estimate_ci_95', 'point_estimate_ci_99'])\n",
    "    CategoricalTestResults = namedtuple('CategoricalTestResults', ['column', 'chi2_stat', 'p_value', 'dof', 'expected', 'observed', 'test_type'])\n",
    "\n",
    "    numerical_results = []\n",
    "    categorical_results = []\n",
    "\n",
    "    for col in numerical_cols + categorical_cols:\n",
    "        if col in numerical_cols:\n",
    "            # Drop any NaN values from consideration\n",
    "            control_col = group_control[col].dropna()\n",
    "            treatment_col = group_treatment[col].dropna()\n",
    "\n",
    "            # Ensure equal sizes\n",
    "            if len(control_col) != len(treatment_col):\n",
    "                print(f\"Column: {col} - Groups are not paired correctly or have missing data!\")\n",
    "                continue\n",
    "\n",
    "            # Calculate the median difference / point estimate for Wilcoxon test\n",
    "            median_diff = np.median(treatment_col - control_col)\n",
    "\n",
    "            # Perform Wilcoxon Signed-Rank Test\n",
    "            stat, p_value = wilcoxon(treatment_col, control_col)\n",
    "\n",
    "            # No direct method for confidence intervals in Wilcoxon test, Perform bootstrapping\n",
    "            ci_bounds = bootstrap_median_difference(treatment_col, control_col, n_bootstrap=1000, confidence_levels=[95, 99])\n",
    "\n",
    "            # Use the number of non-missing paired observations minus one for df\n",
    "            df = len(control_col) - 1\n",
    "\n",
    "            # margin of error calculation:\n",
    "            margin_of_err95 = (ci_bounds[95][1] - ci_bounds[95][0]) / 2\n",
    "            margin_of_err99 = (ci_bounds[99][1] - ci_bounds[99][0]) / 2\n",
    "\n",
    "            # point estimate [CI lower, CI upper]:\n",
    "            formatted_ci_95 = f\"{median_diff}, 95% CI: [{ci_bounds[95][0]}, {ci_bounds[95][1]}]\"\n",
    "            formatted_ci_99 = f\"{median_diff}, 99% CI: [{ci_bounds[99][0]}, {ci_bounds[99][1]}]\"\n",
    "\n",
    "            # Append results with bootstrapped confidence intervals\n",
    "            numerical_results.append(WilcoxonTestResults(col, stat, p_value, df,\n",
    "                                                        ci_bounds[95][0], ci_bounds[95][1],\n",
    "                                                        ci_bounds[99][0], ci_bounds[99][1],\n",
    "                                                        'wilcoxon', median_diff, margin_of_err95, margin_of_err99,\n",
    "                                                        formatted_ci_95, formatted_ci_99\n",
    "                                              ))\n",
    "        elif col in categorical_cols:\n",
    "            # Categorical columns: Chi-squared test\n",
    "            contingency_table = pd.crosstab(dataframe[target_column], dataframe[col])  # contingency table of observed frequencies\n",
    "            chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "            # Add the observed frequencies to your results - new change here\n",
    "            categorical_results.append(CategoricalTestResults(col, chi2_stat, p_value, dof, expected.tolist(), contingency_table.values.tolist(), 'chi-squared'))\n",
    "\n",
    "    # Converting results into DataFrames\n",
    "    numerical_results_df = pd.DataFrame(numerical_results)\n",
    "    categorical_results_df = pd.DataFrame(categorical_results)\n",
    "\n",
    "    # Save the results to an Excel file\n",
    "    with pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n",
    "        numerical_results_df.to_excel(writer, sheet_name='Numerical_Statistics')\n",
    "        categorical_results_df.to_excel(writer, sheet_name='Categorical_Statistics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIkgRmkqISOB"
   },
   "source": [
    "**After Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "si6eEGKqIUYo"
   },
   "outputs": [],
   "source": [
    "# OA Incidence example usage\n",
    "numerical_cols = oa_inc_matched_df.iloc[:,20:-110].columns.tolist()\n",
    "categorical_cols = oa_inc_matched_df.iloc[:,11:20].columns.tolist()\n",
    "calculate_nonparametric_stats_and_save(oa_inc_matched_df, 'oa_prog', numerical_cols, categorical_cols, \"publish_dataframes/OA_Inc_Twins_nonparametric_statistics_output.xlsx\")\n",
    "\n",
    "# TKR example usage\n",
    "numerical_cols = tkr_matched_df.iloc[:,20:-110].columns.tolist()\n",
    "categorical_cols = tkr_matched_df.iloc[:,11:20].columns.tolist()\n",
    "calculate_nonparametric_stats_and_save(tkr_matched_df, 'tkr', numerical_cols, categorical_cols, \"publish_dataframes/TKR_Twins_nonparametric_statistics_output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCWuqpjlIUnv"
   },
   "source": [
    "**Output Statistics**:\n",
    "\n",
    "*OA Incidence*: \"publish_dataframes/OA_Inc_Twins_nonparametric_statistics_output.xlsx\"\n",
    "\n",
    "*TKR*: \"publish_dataframes/TKR_Twins_nonparametric_statistics_output.xlsx\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPWq7xPBqcI0RzrfZd7uWEh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
