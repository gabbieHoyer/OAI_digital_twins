{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgleVll8PSqE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import time\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/Colab_Notebooks/oai/TKR_twin'"
      ],
      "metadata": {
        "id": "j9FClxDOPVD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Fuse OAI + KNOAP Challenge Data**"
      ],
      "metadata": {
        "id": "vA4xAD097kMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set base path for files\n",
        "base_path = '/content/drive/MyDrive/Colab_Notebooks/oai/TKR_twin/'\n",
        "\n",
        "# Read CSV files\n",
        "all_clin = pd.read_csv(base_path + 'OAI_all_knees_data.csv')\n",
        "clean_oai = pd.read_csv(base_path + 'publish_dataframes/pca_modes_and_demos_all_timepoints_12082023.csv')\n",
        "\n",
        "# Process all_clin\n",
        "all_clin[['id', 'side']] = all_clin['Knee'].str.split(\"_\", expand=True)\n",
        "all_clin['side'] = all_clin['side'].replace({'R': 'RIGHT', 'L': 'LEFT'})\n",
        "all_clin['id'] = pd.to_numeric(all_clin['id'])\n",
        "all_clin.drop(columns=['Knee', 'Age', 'Gender', 'BMI', 'Postmeno', 'KL'], inplace=True)\n",
        "\n",
        "# Derive and process tkr from clean_oai\n",
        "tkr = clean_oai[['id', 'side', 'visit', 'total_or_partial']]\n",
        "\n",
        "# Process tkr_ids and tkr_right\n",
        "tkr_ids = tkr[tkr['total_or_partial'].notna()]['id']\n",
        "tkr_right = tkr[tkr['side'] == 'RIGHT']\n",
        "tkr_right_ids = tkr_right[tkr_right['total_or_partial'].notna()]['id']\n",
        "\n",
        "# Merge clean_oai and all_clin into oai_extra, and rename a column\n",
        "oai_extra = pd.merge(clean_oai, all_clin, on=['id', 'side']).rename(columns={'max_kl': 'oa_prog'})\n",
        "\n",
        "# Filter and process right side data\n",
        "right = oai_extra[oai_extra['side'] == 'RIGHT']\n",
        "right['total_or_partial'].fillna(0, inplace=True)\n",
        "right['tkr'] = right['total_or_partial'].apply(lambda x: 0 if x == 0 else 1)\n",
        "\n",
        "# Process baseline_clean_oai\n",
        "baseline_clean_oai = right[right['visit'] == 'V00']\n"
      ],
      "metadata": {
        "id": "B1BzWcfYPVGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Dataframes:**\n",
        "\n",
        "*oai_extra*:\n",
        "'publish_dataframes/oai_knoap_allTimepoints_12082023.csv'\n",
        "\n",
        "*baseline_clean_oai*:\n",
        "'publish_dataframes/oa_incidence_tkr_pc_modes_and_demos_baseline_rightside_12082023.csv'"
      ],
      "metadata": {
        "id": "_OkO6NPPW32X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choose Relevant PC mode, clinical factor, and outcome target columns**"
      ],
      "metadata": {
        "id": "p7yWi-fk7pvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specific columns\n",
        "specific_cols = [\n",
        "    'id', 'side', 'pred_kl', 'KL', 'oa_prog', 'total_or_partial', 'tkr',\n",
        "    'hisp', 'race', 'gender', 'Varus', 'Tenderness', 'Injury_history',\n",
        "    'Mild_symptoms', 'Heberden', 'Crepitus', 'Morning_stiffness',\n",
        "    'age', 'height', 'weight', 'BMI', 'womac_pain', 'womac_adl',\n",
        "    'womac_stiff', 'womac_total', 'koos_pain', 'koos_symptom', 'koos_func', 'koos_qol'\n",
        "]\n",
        "\n",
        "# Pattern-based columns for 'bs', 't2', 'thick'\n",
        "pattern_based_cols_1 = [\n",
        "    f\"{prefix}_{part}_pc{num}\"\n",
        "    for prefix in ['bs', 't2', 'thick']\n",
        "    for part in ['fem', 'pat', 'tib']\n",
        "    for num in range(1, 11)\n",
        "]\n",
        "\n",
        "# Separate pattern for 'med' and 'lat' columns\n",
        "pattern_based_cols_2 = [f\"{prefix}_pc{num}\" for prefix in ['med', 'lat'] for num in range(1, 11)]\n",
        "\n",
        "# Combine the lists\n",
        "all_columns = specific_cols + pattern_based_cols_1 + pattern_based_cols_2\n",
        "\n",
        "# Create the new DataFrame\n",
        "oai_particular = baseline_clean_oai[all_columns]\n",
        "oai_particular.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "VvHn1YxdPVJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Dataframes**:\n",
        "\n",
        "*oai_particular*:\n",
        "'publish_dataframes/stats_matching_targets_demos_pcmodes_12082023.csv'"
      ],
      "metadata": {
        "id": "uv5BKzH6XTWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop rows/samples with missingness in PC mode features**"
      ],
      "metadata": {
        "id": "kVnmtHRZ8I76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the last 110 columns\n",
        "columns_to_check = oai_particular.iloc[:, -110:]\n",
        "\n",
        "# DataFrame with rows dropped if any NaN values in the last 110 columns\n",
        "df_no_missing = oai_particular.dropna(subset=columns_to_check.columns)\n",
        "\n",
        "# DataFrame with only rows that have NaN values in any of the last 110 columns\n",
        "df_with_missing = oai_particular[oai_particular.iloc[:, -110:].isna().any(axis=1)]"
      ],
      "metadata": {
        "id": "tBgXwYkgP03A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Dataframes**:\n",
        "\n",
        "*df_no_missing*:\n",
        "'publish_dataframes/before_matching_no_pcmode_missingness_group.csv'\n",
        "\n",
        "*df_with_missing*:\n",
        "'publish_dataframes/before_matching_pcmode_missingness_group.csv'"
      ],
      "metadata": {
        "id": "1qhbWfftXbWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop columns with more than 5% missingness**"
      ],
      "metadata": {
        "id": "vFgJof2u774p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of missing values per column\n",
        "missing_percentage = df_no_missing.iloc[:,:-110].isnull().mean() * 100\n",
        "\n",
        "# Drop columns with more than 5% missing values\n",
        "columns_to_drop = missing_percentage[missing_percentage > 5].index\n",
        "df_no_missing_dropped = df_no_missing.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "TAwlwwOPP06K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Varus' and 'koos_func' dropped"
      ],
      "metadata": {
        "id": "C3yJaZiKYait"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Multiple Imputattion on missing data by group of target variable**"
      ],
      "metadata": {
        "id": "FjOEilEa7ZWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "\n",
        "def impute_by_group(df, group_col):\n",
        "    # Columns for imputation\n",
        "    categorical_columns = df.columns[7:16]  # Categorical variable columns for imputation\n",
        "    continuous_columns = df.columns[16:-110]  # Continuous variable columns for imputation\n",
        "\n",
        "    # Initialize IterativeImputer for continuous and categorical data\n",
        "    imputer_cont = IterativeImputer(estimator=RandomForestRegressor(), initial_strategy='median', max_iter=40, random_state=0)\n",
        "    imputer_cat = IterativeImputer(estimator=RandomForestClassifier(), initial_strategy='most_frequent', max_iter=40, random_state=0)\n",
        "\n",
        "    # Empty list to hold imputed parts\n",
        "    imputed_parts = []\n",
        "\n",
        "    # Iterate over each group in specified column\n",
        "    for name, group in df.groupby(group_col):\n",
        "        # Separate categorical and continuous data\n",
        "        categorical_data = group[categorical_columns]\n",
        "        continuous_data = group[continuous_columns]\n",
        "\n",
        "        # Impute continuous and categorical data\n",
        "        continuous_data_imputed = imputer_num.fit_transform(continuous_data)\n",
        "        categorical_data_imputed = imputer_cat.fit_transform(categorical_data)\n",
        "\n",
        "        # Convert imputed data back to DataFrame\n",
        "        continuous_data_imputed_df = pd.DataFrame(continuous_data_imputed, columns=continuous_columns, index=group.index)\n",
        "        categorical_data_imputed_df = pd.DataFrame(categorical_data_imputed, columns=categorical_columns, index=group.index)\n",
        "\n",
        "        # Combine imputed data with non-imputed data\n",
        "        combined_data = pd.concat([group.iloc[:,:7], categorical_data_imputed_df, continuous_data_imputed_df, group.iloc[:,-110:]], axis=1)\n",
        "\n",
        "        # Append combined data to the list\n",
        "        imputed_parts.append(combined_data)\n",
        "\n",
        "    # Concatenate all parts into one DataFrame and sort by index\n",
        "    return pd.concat(imputed_parts).sort_index()\n",
        "\n",
        "# df_no_missing_dropped is your DataFrame\n",
        "df = df_no_missing_dropped.copy()\n",
        "\n",
        "# Use the function for 'tkr' column\n",
        "df_imputed_tkr = impute_by_group(df, 'tkr')\n",
        "\n",
        "# Use the function for 'oa_prog' column\n",
        "df_imputed_oa_inc = impute_by_group(df, 'oa_prog')\n"
      ],
      "metadata": {
        "id": "13QV6UDbYHjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Dataframes**:\n",
        "\n",
        "*df_imputed_oa_inc*:\n",
        "'publish_dataframes/oa_inc_multiple_imputation_filled.csv'\n",
        "\n",
        "*df_imputed_tkr*:\n",
        "'publish_dataframes/tkr_multiple_imputation_filled.csv'"
      ],
      "metadata": {
        "id": "tgQwzuVwWShg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sensitivity Analysis - comparing the distributions of variables before and after multiple imputation**"
      ],
      "metadata": {
        "id": "7JLaNdh48arY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "def ks_test_by_group_to_excel(original_df, imputed_df, columns_to_test, group_col, excel_file_path):\n",
        "    # Initialize a list to store test results\n",
        "    ks_test_results = []\n",
        "\n",
        "    # Iterate over each group in the specified column\n",
        "    for name, group in original_df.groupby(group_col):\n",
        "        group_imputed = imputed_df[imputed_df[group_col] == name]\n",
        "\n",
        "        for column in columns_to_test:\n",
        "            original_data = group[column].dropna()  # Original data for the column in the group\n",
        "            imputed_data = group_imputed[column]    # Imputed data for the column in the group\n",
        "\n",
        "            # Conduct the Kolmogorov-Smirnov test\n",
        "            ks_statistic, p_value = ks_2samp(original_data, imputed_data)\n",
        "\n",
        "            # Append the results to the list\n",
        "            ks_test_results.append({\n",
        "                'Group': name,\n",
        "                'Column': column,\n",
        "                'KS Statistic': ks_statistic,\n",
        "                'P-Value': p_value\n",
        "            })\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    results_df = pd.DataFrame(ks_test_results)\n",
        "\n",
        "    # Save the results to an Excel file\n",
        "    results_df.to_excel(excel_file_path, index=False)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# df is the original DataFrame and df_imputed_tkr is the DataFrame after imputation\n",
        "# Columns to test: 7:-110 in df_imputed\n",
        "columns_to_test = df_imputed_tkr.columns[7:-110].tolist()\n",
        "\n",
        "# File path for the Excel output\n",
        "excel_file_path = 'publish_dataframes/tkr_ks_test_results.xlsx'\n",
        "\n",
        "# Use the function for 'tkr' column and save results to Excel\n",
        "ks_results_tkr = ks_test_by_group_to_excel(df, df_imputed_tkr, columns_to_test, 'tkr', excel_file_path)\n",
        "\n",
        "# Print a preview of the results\n",
        "print(ks_results_tkr.head())\n"
      ],
      "metadata": {
        "id": "TFUP-dCdYHmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OA Incidence Target Column:\n",
        "\n",
        "columns_to_test = df_imputed_oa_inc.columns[7:-110].tolist()\n",
        "\n",
        "# File path for the Excel output\n",
        "excel_file_path = 'publish_dataframes/oa_inc_ks_test_results.xlsx'\n",
        "\n",
        "# Use the function for 'tkr' column and save results to Excel\n",
        "ks_results_oa_inc = ks_test_by_group_to_excel(df, df_imputed_oa_inc, columns_to_test, 'oa_prog', excel_file_path)\n",
        "\n",
        "# Print a preview of the results\n",
        "print(ks_results_oa_inc.head())"
      ],
      "metadata": {
        "id": "MTtNEgQLI2Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Statistics**:\n",
        "\n",
        "*ks_results_oa_inc*: 'publish_dataframes/oa_inc_ks_test_results.xlsx'\n",
        "\n",
        "*ks_results_tkr*: 'publish_dataframes/tkr_ks_test_results.xlsx'\n"
      ],
      "metadata": {
        "id": "MFv2Ur5CY2VP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standardizing Data**"
      ],
      "metadata": {
        "id": "I7xoqhm88773"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def scale_columns(df, start_col_idx, end_col_idx):\n",
        "    \"\"\"\n",
        "    Scales specified columns of a dataframe based on given column indices.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The dataframe to be scaled.\n",
        "    start_col_idx (int): The starting index of the columns to be scaled.\n",
        "    end_col_idx (int): The ending index of the columns to be scaled (exclusive).\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A copy of the dataframe with specified columns scaled.\n",
        "    \"\"\"\n",
        "    cols_to_scale = df.columns[start_col_idx:end_col_idx]\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    sc.fit(df[cols_to_scale])\n",
        "\n",
        "    transformed_df = df.copy()\n",
        "    transformed_df[cols_to_scale] = sc.transform(transformed_df[cols_to_scale])\n",
        "\n",
        "    return transformed_df\n",
        "\n",
        "# Example usage for df_imputed_tkr\n",
        "transformed_tkr = scale_columns(df_imputed_tkr, 16, -110)\n",
        "\n",
        "# Adjust the indices as per the dataframe structure\n",
        "transformed_oa_inc = scale_columns(df_imputed_oa_inc, 16, -110)\n"
      ],
      "metadata": {
        "id": "eTGV9ughYHpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only looking at patients who did not have oa to start with but may progress\n",
        "\n",
        "transformed_oa_inc_control = transformed_oa_inc[(transformed_oa_inc['pred_kl']==0) | (transformed_oa_inc['pred_kl']==1)]\n",
        "\n",
        "transformed_oa_inc_control['pred_kl'].unique()   # array([1., 0.])"
      ],
      "metadata": {
        "id": "2ZEtbbw2Bou8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Dataframes**:\n",
        "\n",
        "*transformed_oa_inc_control*:\n",
        "'publish_dataframes/control_oa_inc_standardized_df.csv'\n",
        "\n",
        "\n",
        "\n",
        "*transformed_tkr*: 'publish_dataframes/tkr_standardized_df.csv'\n"
      ],
      "metadata": {
        "id": "J_ShBRsMcFpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Dataframes for Matching**"
      ],
      "metadata": {
        "id": "yMgW8_G3j3bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def prepare_dataframe_for_matching(dataframe, target_flag_name, covariates_range, id_column_name):\n",
        "    \"\"\"\n",
        "    Prepares a DataFrame for matching by selecting the relevant columns.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The original DataFrame.\n",
        "    target_flag_name (str): The name of the column containing the target flag.\n",
        "    covariates_range (slice): The range of columns to be used as covariates.\n",
        "    id_column_name (str): The name of the column containing the ID.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A new DataFrame with the selected columns.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select the covariates and the target flag\n",
        "    covariates = dataframe.iloc[:, covariates_range]\n",
        "    target_flag = dataframe[target_flag_name]\n",
        "    id_column = dataframe[id_column_name]\n",
        "\n",
        "    # Create a new DataFrame with only the relevant columns\n",
        "    prepared_dataframe = pd.concat([id_column, target_flag, covariates], axis=1)\n",
        "\n",
        "    return prepared_dataframe\n",
        "\n",
        "# Example usage\n",
        "tkr_covariate_df = prepare_dataframe_for_matching(transformed_tkr, 'tkr', slice(7, -110), 'id')\n",
        "\n",
        "# control at baseline - no oa at baseline group\n",
        "oa_inc_covariate_df = prepare_dataframe_for_matching(transformed_oa_inc_control, 'oa_prog', slice(7, -110), 'id')\n",
        "\n",
        "# tkr_covariate_df.reset_index(inplace=True, drop=True)\n",
        "# oa_inc_covariate_df.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "CaoKCpFl8_5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Dataframes**:\n",
        "\n",
        "*oa_inc_covariate_df*: 'publish_dataframes/oa_inc_covariate_df.csv'\n",
        "\n",
        "\n",
        "*tkr_covariate_df*: 'publish_dataframes/tkr_covariate_df.csv'"
      ],
      "metadata": {
        "id": "CNv9pyYLj-f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tkr_covariate_df['id'] = tkr_covariate_df['id'].astype('str')\n",
        "oa_inc_covariate_df['id'] = oa_inc_covariate_df['id'].astype('str')"
      ],
      "metadata": {
        "id": "X4a-svUDJpxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**R in Python**"
      ],
      "metadata": {
        "id": "ZES9SzzF7cg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade rpy2==3.5.1"
      ],
      "metadata": {
        "id": "0JZJ6-wA8_7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rpy2.robjects import r"
      ],
      "metadata": {
        "id": "Jn06X8LQ3n21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "Uh2Fnpp53n5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# install.packages(\"MatchIt\", repos=\"http://cran.us.r-project.org\")\n",
        "# install.packages(\"Matching\", repos=\"http://cran.us.r-project.org\")\n",
        "# install.packages(\"rgenoud\", repos=\"http://cran.us.r-project.org\")"
      ],
      "metadata": {
        "id": "a66xa1oC3n8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_distance(input_dataframe, method='euclidean'):\n",
        "    # Add logic to select only covariate columns if needed\n",
        "    # Example: input_dataframe = input_dataframe.iloc[:, 2:]\n",
        "\n",
        "    # Convert the DataFrame to CSV for R\n",
        "    temp_covariates_csv = 'temp_covariates_data.csv'\n",
        "    input_dataframe.to_csv(temp_covariates_csv, index=False)\n",
        "\n",
        "    # R script to calculate distance\n",
        "    r_script = f'''\n",
        "    df_covariates <- read.csv(\"{temp_covariates_csv}\")\n",
        "    euclidean_dist_matrix <- as.matrix(dist(df_covariates, method = \"{method}\"))\n",
        "    '''\n",
        "\n",
        "    # Execute R code\n",
        "    r(r_script)\n",
        "\n",
        "    # Return the name of the R object holding the distance matrix\n",
        "    return 'euclidean_dist_matrix'"
      ],
      "metadata": {
        "id": "N61FaVys3n_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def perform_matching(input_dataframe, target_column, output_csv, output_matched_csv, method='nearest', params=None,  distance_method=None):\n",
        "    print(\"Starting matching process...\")\n",
        "\n",
        "    # Convert IDs to string\n",
        "    input_dataframe['id'] = input_dataframe['id'].astype('str')\n",
        "\n",
        "    # Save the DataFrame as a CSV file for R\n",
        "    temp_csv = 'temp_input_data.csv'\n",
        "    input_dataframe.to_csv(temp_csv, index=False)\n",
        "    print(\"Dataframe saved as CSV for R.\")\n",
        "\n",
        "    # Prepare the parameters string from the dictionary\n",
        "    params_string = \", \".join([f\"{key} = {value}\" for key, value in params.items()]) if params else \"\"\n",
        "\n",
        "    # Check if a specific distance calculation is needed\n",
        "    if distance_method:\n",
        "        distance_matrix_var = calculate_distance(input_dataframe, method=distance_method)\n",
        "        params_string += f\", distance = {distance_matrix_var}\" if params_string else f\"distance = {distance_matrix_var}\"\n",
        "\n",
        "    # Prepare the R script with dynamic target column and custom parameters\n",
        "    r_script = f'''\n",
        "    library(MatchIt)\n",
        "    library(rgenoud)\n",
        "    library(Matching)\n",
        "\n",
        "    print(\"R libraries loaded. Reading data into R...\")\n",
        "\n",
        "    # Read the data into R\n",
        "    input_data_r <- read.csv(\"{temp_csv}\")\n",
        "\n",
        "    print(\"Data read into R. Starting matching process with method '{method}' and custom parameters...\")\n",
        "\n",
        "    # Perform matching using MatchIt with specified method and custom parameters\n",
        "    m.out <- matchit({target_column} ~ ., data = input_data_r, method = \"{method}\", {params_string})\n",
        "\n",
        "    print(\"Matching completed. Retrieving matched data...\")\n",
        "\n",
        "    # Retrieve the matched data with IDs\n",
        "    matched_data <- get_matches(m.out, id = \"new_id\")\n",
        "\n",
        "    print(\"Matched data retrieved. Writing to CSV...\")\n",
        "\n",
        "    # Write the matched data to a CSV file\n",
        "    write.csv(matched_data, \"{output_matched_csv}\", row.names = FALSE)\n",
        "    '''\n",
        "\n",
        "    # Execute R code including library loading\n",
        "    print(\"Executing R code for matching...\")\n",
        "    r(r_script)\n",
        "\n",
        "    print(\"R processing completed. Reading matched data back into Pandas...\")\n",
        "\n",
        "    # Read the matched data back into a Pandas DataFrame\n",
        "    matched_data_df = pd.read_csv(output_matched_csv)\n",
        "    matched_data_df.to_csv(output_csv, index=False)\n",
        "\n",
        "    print(\"Matching process completed.\")"
      ],
      "metadata": {
        "id": "BnfoyW1v30kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nearest Neighbor - Propensity Score Example\n",
        "input_df = pd.read_csv('publish_dataframes/tkr_covariate_df.csv')\n",
        "output_csv = 'publish_dataframes/test_tkr_NN_noRep_df.csv'\n",
        "output_matched_csv = 'publish_dataframes/tkr_temp_matched_data.csv'\n",
        "\n",
        "perform_matching(input_df, 'tkr', output_csv, output_matched_csv,\n",
        "                 method='nearest')\n",
        "\n",
        "#**************************************\n",
        "\n",
        "# Genetic Twins Example\n",
        "# input_df = pd.read_csv('publish_dataframes/tkr_covariate_df.csv')\n",
        "# output_csv = 'publish_dataframes/tkr_Genetic_Twins_noRep_df.csv'\n",
        "# output_matched_csv = 'publish_dataframes/tkr_temp_matched_data.csv'\n",
        "\n",
        "# perform_matching(input_df, 'tkr', output_csv, output_matched_csv)\n",
        "\n",
        "#**************************************\n",
        "\n",
        "# CEM Example\n",
        "# Specify CEM parameters\n",
        "# cem_params = {\n",
        "#     'cuts': 30,\n",
        "#     'M': 1.0,\n",
        "#     'weighting': 'TRUE',\n",
        "# }\n",
        "# perform_matching2(input_df, 'tkr', output_csv, output_matched_csv,\n",
        "#                  method='cem', params=cem_params)\n",
        "\n",
        "#**************************************\n",
        "\n",
        "# Euclidean Distance with Replacement Example\n",
        "# params_dict = {'replace': True}\n",
        "# perform_matching(input_df, 'tkr', output_csv, output_matched_csv,\n",
        "#                  method='nearest', params=params_dict, distance_method='euclidean')\n"
      ],
      "metadata": {
        "id": "P2uatHFO33lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Dataframes**:\n",
        "\n",
        "*nearest neighbor, no replacement*:\n",
        "'publish_dataframes/oa_inc_matchit_nearestNeightborMethod_noReplacement_df.csv'\n",
        "'publish_dataframes/tkr_matchit_nearestNeightborMethod_noReplacement_df.csv'\n",
        "\n",
        "*nearest neighbor, replacement*:\n",
        "'publish_dataframes/oa_inc_matchit_nearestNeightborMethod_Replacement_df.csv'\n",
        "'publish_dataframes/tkr_matchit_nearestNeightborMethod_Replacement_df.csv'\n",
        "\n",
        "\n",
        "*euclidean distance matrix, replacement*:\n",
        "'publish_dataframes/oa_inc_matchit_EuclideanDistanceMatrixMethod_Replacement_df.csv'\n",
        "'publish_dataframes/tkr_matchit_EuclideanDistanceMatrixMethod_Replacement_df.csv'\n",
        "\n",
        "*euclidean distance matrix, no replacement*:\n",
        "'publish_dataframes/oa_inc_matchit_EuclideanDistanceMatrixMethod_noReplacement_df.csv'\n",
        "'publish_dataframes/tkr_matchit_EuclideanDistanceMatrixMethod_noReplacement_df.csv'"
      ],
      "metadata": {
        "id": "5UVEudfdJ0p1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen Matchit Method: Nearest Neighbors, no Replacement - matches found via Propensity Score"
      ],
      "metadata": {
        "id": "PjmMFerazup0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Or the TSNE 3D dimensionality reduction + Euclidean Distance**"
      ],
      "metadata": {
        "id": "n_iCuuQqWtwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_tsne(df, tsne_cols, tsne_params):\n",
        "    \"\"\"\n",
        "    Performs t-SNE transformation on the specified columns of the DataFrame.\n",
        "\n",
        "    :param df: DataFrame to be processed.\n",
        "    :param tsne_cols: Columns to be used for t-SNE transformation (as a slice object).\n",
        "    :param tsne_params: Dictionary of parameters for t-SNE.\n",
        "    :return: DataFrame with t-SNE components added.\n",
        "    \"\"\"\n",
        "    tsne = TSNE(**tsne_params)\n",
        "    tsne_components = tsne.fit_transform(df.iloc[:, tsne_cols])\n",
        "    tsne_df = df.copy()\n",
        "    tsne_df[['tsne-one', 'tsne-two', 'tsne-three']] = tsne_components\n",
        "    return tsne_df\n"
      ],
      "metadata": {
        "id": "a_u__PET8j53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distances(df, id_col):\n",
        "    \"\"\"\n",
        "    Calculates the pairwise Euclidean distances between rows in the DataFrame.\n",
        "\n",
        "    :param df: DataFrame with t-SNE components.\n",
        "    :param id_col: The column name that uniquely identifies each row.\n",
        "    :return: DataFrame of distances.\n",
        "    \"\"\"\n",
        "    dist_df = pd.DataFrame(\n",
        "        squareform(pdist(df[['tsne-one', 'tsne-two', 'tsne-three']])),\n",
        "        columns=df[id_col].unique(),\n",
        "        index=df[id_col].unique()\n",
        "    )\n",
        "    return dist_df\n"
      ],
      "metadata": {
        "id": "JwsfoslPWxXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_pairs(df, target_col, dist_df, id_col):\n",
        "    \"\"\"\n",
        "    Identifies the closest pairs in the dataset.\n",
        "\n",
        "    :param df: Original DataFrame with target information.\n",
        "    :param target_col: Target column name.\n",
        "    :param dist_df: DataFrame with distances.\n",
        "    :param id_col: The column name that uniquely identifies each row.\n",
        "    :return: DataFrame with closest pair information.\n",
        "    \"\"\"\n",
        "    target_ids = df[df[target_col] == 1][id_col].tolist()\n",
        "    progressing_dist = dist_df[target_ids]\n",
        "\n",
        "    # Creating Long Format DataFrame\n",
        "    progressing_long = progressing_dist.stack().reset_index().rename(columns={'level_0':'id','level_1':'pt2',0:'distance'})\n",
        "\n",
        "    # Merging with Target Column Information\n",
        "    target_vals = df[['id', target_col]]\n",
        "    target_long = progressing_long.merge(target_vals, on='id').rename(columns={'id':'pt1_id', 'pt2':'id', target_col: target_col + '_pt1'})\n",
        "    target_2pat = target_long.merge(target_vals, on='id').rename(columns={'id':'pt2_id', target_col: target_col + '_pt2'})\n",
        "\n",
        "    # Identifying Different Pairs\n",
        "    min_sort_target = (target_2pat.sort_values(['pt1_id','distance'], ascending=True)[['pt1_id','pt2_id','distance', target_col+'_pt1', target_col+'_pt2']])\n",
        "    min_sort_target['diff_pair'] = (min_sort_target[target_col+'_pt1'] != min_sort_target[target_col+'_pt2'])\n",
        "\n",
        "    # Finding the closest different pairs for each 'pt2_id'\n",
        "    closest_pairs = min_sort_target[min_sort_target['diff_pair']].sort_values('distance').groupby('pt2_id').first().reset_index()\n",
        "\n",
        "    # Dropping the 'diff_pair' column as it's no longer needed\n",
        "    closest_pairs = closest_pairs.drop('diff_pair', axis=1)\n",
        "\n",
        "    # Prepare for melting: Identify columns ending with '_id'\n",
        "    id_cols = [col for col in closest_pairs.columns if col.endswith('_id')]\n",
        "\n",
        "    # Melting the DataFrame to long format\n",
        "    twins_melt = pd.melt(closest_pairs, id_vars='distance', value_vars=id_cols, value_name='id')\n",
        "\n",
        "    # Assigning 'tkr' values based on the 'variable' column\n",
        "    twins_melt[target_col] = np.where(twins_melt['variable'] == 'pt1_id', 0, 1)\n",
        "\n",
        "    # Dropping the 'variable' column as it's no longer needed\n",
        "    twins_melt = twins_melt.drop('variable', axis=1)\n",
        "\n",
        "    return twins_melt\n"
      ],
      "metadata": {
        "id": "T5A09x9tWxdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_params = {'n_components': 3, 'perplexity': 50, 'n_iter': 4000, 'random_state' : 42}\n",
        "\n",
        "transformed_oa_inc_control = pd.read_csv('publish_dataframes/control_oa_inc_standardized_df.csv')\n",
        "\n",
        "df = transformed_oa_inc_control\n",
        "target_col = 'oa_prog'    # target column\n",
        "tsne_cols = slice(7, 27)  # column slice for t-SNE\n",
        "\n",
        "# Perform t-SNE transformation\n",
        "tsne_df = perform_tsne(df, tsne_cols, tsne_params)\n",
        "\n",
        "# Calculate distances\n",
        "dist_df = calculate_distances(tsne_df, 'id')\n",
        "\n",
        "# Find closest pairs\n",
        "matched_oa_inc_df = find_closest_pairs(df, target_col, dist_df, 'id')\n",
        "\n",
        "# matched_oa_inc_df.to_csv('publish_dataframes/oa_inc_matchit_TSNE_EuclideanDist_Replacement_df.csv', index=False)\n"
      ],
      "metadata": {
        "id": "I3BcPnYwW03L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_params = {'n_components': 3, 'perplexity': 65, 'n_iter': 4000, 'random_state' : 42}  # perp is sqrt(N) -> sqrt(4283) ~ 65\n",
        "\n",
        "transformed_tkr = pd.read_csv('publish_dataframes/tkr_standardized_df.csv')\n",
        "\n",
        "df = transformed_tkr\n",
        "target_col = 'tkr'    # target column\n",
        "tsne_cols = slice(7, 27)  # column slice for t-SNE\n",
        "\n",
        "# Perform t-SNE transformation\n",
        "tsne_df = perform_tsne(df, tsne_cols, tsne_params)\n",
        "\n",
        "# Calculate distances\n",
        "dist_df = calculate_distances(tsne_df, 'id')\n",
        "\n",
        "# Find closest pairs\n",
        "matched_tkr_df = find_closest_pairs(df, target_col, dist_df, 'id')\n",
        "\n",
        "# matched_tkr_df.to_csv('publish_dataframes/tkr_matchit_TSNE_EuclideanDist_Replacement_df.csv', index=False)"
      ],
      "metadata": {
        "id": "isTFJy5QW4EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Dataframes**:\n",
        "\n",
        "*matched_oa_inc_df*:\n",
        "'publish_dataframes/rand_state_oa_inc_matchit_TSNE_EuclideanDist_Replacement_df.csv'\n",
        "\n",
        "*matched_tkr_df*:\n",
        "\n",
        "'publish_dataframes/rand_state_tkr_matchit_TSNE_EuclideanDist_Replacement_df.csv'"
      ],
      "metadata": {
        "id": "8VmQ82cAW5t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OA Inc matched cohort size:\n",
        "# matched_oa_inc_df[matched_oa_inc_df['oa_prog']==0]['id'].nunique()\n",
        "# matched_oa_inc_df[matched_oa_inc_df['oa_prog']==1]['id'].nunique()\n",
        "# control: 319\n",
        "# oa inc: 357\n",
        "\n",
        "# TKR matched cohort size:\n",
        "# matched_tkr_df[matched_tkr_df['tkr']==0]['id'].nunique()\n",
        "# matched_tkr_df[matched_tkr_df['tkr']==1]['id'].nunique()\n",
        "# control: 233\n",
        "# tkr: 253"
      ],
      "metadata": {
        "id": "Ulay30ZjDWf6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}