{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMm+C+EhO4vpEtbmOxvznPH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install openpyxl\n","!pip install xlsxwriter"],"metadata":{"id":"U8Sj-ZoZa_7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ygqdsoUeP2o"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import openpyxl\n","import xlsxwriter\n","\n","import statsmodels.api as sm\n","from scipy.stats import ttest_rel, t\n","from scipy.stats import shapiro, anderson, wilcoxon\n","from scipy.stats import ttest_rel, false_discovery_control\n","\n","from collections import namedtuple\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/Colab_Notebooks/oai/TKR_twin'"],"metadata":{"id":"NQUrrytweSQD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the random seed for reproducibility\n","np.random.seed(42)"],"metadata":{"id":"T3bo4rCZjksI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Combine Matched Subject IDs with PC modes**"],"metadata":{"id":"F5lwob7rgUO8"}},{"cell_type":"code","source":["\n","def load_and_merge_data(full_data_path, match_data_path, merge_key, selected_columns):\n","    full_df = pd.read_csv(full_data_path)\n","    match_df = pd.read_csv(match_data_path, usecols=selected_columns)\n","    merged_df = pd.merge(match_df, full_df, on=merge_key, how='left')\n","    return merged_df\n","\n","selected_columns = ['distance','id']\n","oa_inc_matched_df = load_and_merge_data(\n","    'publish_dataframes/oa_inc_multiple_imputation_filled.csv',\n","    'publish_dataframes/rand_state_oa_inc_matchit_TSNE_EuclideanDist_Replacement_df.csv',\n","    'id',\n","    selected_columns\n",")\n","# oa_inc_matched_df\n","\n","tkr_matched_df = load_and_merge_data(\n","    'publish_dataframes/tkr_multiple_imputation_filled.csv',\n","    'publish_dataframes/rand_state_tkr_matchit_TSNE_EuclideanDist_Replacement_df.csv',\n","    'id',\n","    selected_columns\n",")\n","\n","# Example usage\n","# selected_columns = ['new_id', 'subclass', 'weights', 'distance', 'id']\n","# oa_inc_matched_df = load_and_merge_data(\n","#     'publish_dataframes/oa_inc_multiple_imputation_filled.csv',\n","#     'publish_dataframes/oa_inc_matchit_nearestNeightborMethod_noReplacement_df.csv',\n","#     'id',\n","#     selected_columns\n","# )\n","\n","# tkr_matched_df = load_and_merge_data(\n","#     'publish_dataframes/tkr_multiple_imputation_filled.csv',\n","#     'publish_dataframes/tkr_matchit_nearestNeightborMethod_noReplacement_df.csv',\n","#     'id',\n","#     selected_columns\n","# )\n","\n","# oa_inc_matched_df.to_csv('publish_dataframes/oa_inc_matched_IDs_PC_modes.csv', index=False)\n","# tkr_matched_df.to_csv('publish_dataframes/tkr_matched_IDs_PC_modes.csv', index=False)"],"metadata":{"id":"u-8P48fheSS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Output Dataframes**:\n","\n","*oa_inc_matched_df*: 'publish_dataframes/oa_inc_matched_IDs_PC_modes.csv'\n","\n","*tkr_matched_df*: 'publish_dataframes/tkr_matched_IDs_PC_modes.csv'"],"metadata":{"id":"e3OtFWRYhKCG"}},{"cell_type":"markdown","source":["### **Evaluating Imaging Biomarker PC Modes**"],"metadata":{"id":"JnGxyf1bHBgC"}},{"cell_type":"markdown","source":["**Normality and Homoscedasticity Testing**"],"metadata":{"id":"eJHv1Q8t8cJ3"}},{"cell_type":"code","source":["def perform_normality_tests(dataframe, group_column, column_range):\n","    # Extracting groups\n","    group_control = dataframe[dataframe[group_column] == 0].reset_index(drop=True)\n","    group_treatment = dataframe[dataframe[group_column] == 1].reset_index(drop=True)\n","\n","    # Ensure equal sizes\n","    assert len(group_control) == len(group_treatment), \"Groups are not paired correctly!\"\n","\n","    # Define a DataFrame to hold our results\n","    results = pd.DataFrame(columns=['variable', 'shapiro_stat', 'shapiro_p', 'anderson_stat', 'anderson_critical_values', 'anderson_significance_level'])\n","\n","    # Determine the slicing range\n","    if isinstance(column_range, tuple):\n","        start, end = column_range\n","        selected_columns = dataframe.iloc[:, start:end if end is not None else None].columns\n","    elif isinstance(column_range, slice):\n","        selected_columns = dataframe.iloc[:, column_range].columns\n","    else:\n","        raise ValueError(\"column_range must be a slice or a tuple\")\n","\n","    # Loop through all variable columns\n","    for col in selected_columns:\n","        differences = group_treatment[col] - group_control[col]\n","\n","        # Drop NA values from differences\n","        differences = differences.dropna()\n","\n","        # Shapiro-Wilk Test\n","        shapiro_stat, shapiro_p = shapiro(differences)\n","\n","        # Anderson-Darling Test\n","        anderson_result = anderson(differences)\n","        anderson_stat = anderson_result.statistic\n","        anderson_critical_values = anderson_result.critical_values\n","        anderson_significance_level = anderson_result.significance_level\n","\n","        # Append results\n","        results = results.append({\n","            'variable': col,\n","            'shapiro_stat': shapiro_stat,\n","            'shapiro_p': shapiro_p,\n","            'anderson_stat': anderson_stat,\n","            'anderson_critical_values': anderson_critical_values,\n","            'anderson_significance_level': anderson_significance_level\n","        }, ignore_index=True)\n","\n","    return results\n"],"metadata":{"id":"yDew95cs8mtA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Checking Normality and Homoscedasticity of Twin/Matched Subject imaging biomarker PC modes"],"metadata":{"id":"rPChzJUNi7-I"}},{"cell_type":"code","source":["# Example usage for oa_inc_matched_df\n","oa_inc_results = perform_normality_tests(oa_inc_matched_df, 'oa_prog', (-110, None))\n","\n","# Example usage for tkr_matched_df\n","tkr_results = perform_normality_tests(tkr_matched_df, 'tkr', (-110, None))\n","\n","# Optionally, save the results to Excel files\n","# oa_inc_results.to_excel('publish_dataframes/OA_Inc_Clinical_Twin_PCA_variables_normality_tests_results.xlsx', index=False, engine='openpyxl')\n","# tkr_results.to_excel('publish_dataframes/TKR_Clinical_Twin_PCA_variables_normality_tests_results.xlsx', index=False, engine='openpyxl')\n"],"metadata":{"id":"EIMd7vKJ8xRP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Output Statistics**:\n","\n","*oa_inc_results*: 'publish_dataframes/OA_Inc_Clinical_Twin_PCA_variables_normality_tests_results.xlsx'\n","\n","\n","*tkr_results*:\n","'publish_dataframes/TKR_Clinical_Twin_PCA_variables_normality_tests_results.xlsx'"],"metadata":{"id":"v1qo2rTbldYd"}},{"cell_type":"markdown","source":["**Non-parametric Statistical Hypothesis Testing**"],"metadata":{"id":"9Gbl4_7dmMVt"}},{"cell_type":"code","source":["\n","def bootstrap_median_difference(treatment_col, control_col, n_bootstrap=1000, confidence_levels=[95, 99], seed=None):\n","    \"\"\"\n","    Performs bootstrap analysis to estimate confidence intervals of the median differences between two columns.\n","\n","    Parameters:\n","    treatment_col (array-like): The treatment group column.\n","    control_col (array-like): The control group column.\n","    n_bootstrap (int): The number of bootstrap samples to draw.\n","    confidence_levels (list): The confidence levels for which to compute the intervals.\n","    seed (int, optional): The seed for the random number generator.\n","\n","    Returns:\n","    dict: Confidence intervals for each specified level.\n","    \"\"\"\n","    if seed is not None:\n","        np.random.seed(seed)\n","\n","    if len(treatment_col) != len(control_col):\n","        raise ValueError(\"Treatment and control columns must be of the same length.\")\n","\n","    bootstrapped_medians = []\n","    for _ in range(n_bootstrap):\n","        resampled_oa = np.random.choice(treatment_col, size=len(treatment_col), replace=True)\n","        resampled_control = np.random.choice(control_col, size=len(control_col), replace=True)\n","        median_diff = np.median(resampled_oa - resampled_control)\n","        bootstrapped_medians.append(median_diff)\n","\n","    ci_bounds = {}\n","    for level in confidence_levels:\n","        if not (0 < level < 100):\n","            raise ValueError(\"Confidence levels must be between 0 and 100.\")\n","        lower_bound = np.percentile(bootstrapped_medians, (100 - level) / 2)\n","        upper_bound = np.percentile(bootstrapped_medians, 100 - (100 - level) / 2)\n","        ci_bounds[level] = (lower_bound, upper_bound)\n","\n","    return ci_bounds\n"],"metadata":{"id":"HJSUlOSPeSYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def perform_statistical_tests(dataframe, group_column, variable_columns_range, non_norm_columns):\n","    # Extracting groups\n","    group_control = dataframe[dataframe[group_column] == 0].reset_index(drop=True)\n","    group_treatment = dataframe[dataframe[group_column] == 1].reset_index(drop=True)\n","\n","    # Ensure equal sizes\n","    assert len(group_control) == len(group_treatment), \"Groups are not paired correctly!\"\n","\n","    # Define named tuples for results\n","    TestResults = namedtuple('TestResults', ['column', 't_stat', 'p_value', 'adj_p_value', 'df', 'ci_95_low', 'ci_95_high', 'ci_99_low', 'ci_99_high', 'test_type', 'mean_diff', 'margin_of_err95', 'margin_of_err99', 'point_estimate_ci_95', 'point_estimate_ci_99'])\n","    WilcoxonTestResults = namedtuple('WilcoxonTestResults', ['column', 'wilcoxon_stat', 'p_value', 'adj_p_value', 'df', 'ci_95_low', 'ci_95_high', 'ci_99_low', 'ci_99_high', 'test_type', 'median_diff', 'margin_of_err95', 'margin_of_err99', 'point_estimate_ci_95', 'point_estimate_ci_99'])\n","\n","    results = []\n","    numerical_p_values = []\n","\n","    # Loop through variable columns\n","    for col in dataframe.iloc[:, variable_columns_range:].columns:\n","        control_col = group_control[col].dropna()\n","        treatment_col = group_treatment[col].dropna()\n","\n","        # Ensure equal sizes\n","        if len(control_col) != len(treatment_col):\n","            print(f\"Column: {col} - Groups are not paired correctly or have missing data!\")\n","            continue\n","\n","        # Determine if we should use t-test or Wilcoxon test\n","        if col in non_norm_columns:\n","\n","            # Calculate the median difference / point estimate for Wilcoxon test\n","            median_diff = np.median(treatment_col - control_col)\n","\n","            # Perform Wilcoxon Signed-Rank Test instead of t-test\n","            stat, p_value = wilcoxon(treatment_col, control_col)\n","            # No direct method for confidence intervals in Wilcoxon test, consider using a bootstrapping method if needed\n","\n","            # Perform bootstrapping for confidence intervals\n","            ci_bounds = bootstrap_median_difference(treatment_col, control_col, n_bootstrap=1000, confidence_levels=[95, 99])\n","\n","            # Use the number of non-missing paired observations minus one for df\n","            df = len(control_col) - 1\n","\n","            # Accumulate p-values for Wilcoxon tests\n","            numerical_p_values.append(p_value)\n","\n","            # margin of error calculation:\n","            margin_of_err95 = (ci_bounds[95][1] - ci_bounds[95][0]) / 2\n","            margin_of_err99 = (ci_bounds[99][1] - ci_bounds[99][0]) / 2\n","\n","            # point estimate [CI lower, CI upper]:\n","            formatted_ci_95 = f\"{median_diff}, 95% CI: [{ci_bounds[95][0]}, {ci_bounds[95][1]}]\"\n","            formatted_ci_99 = f\"{median_diff}, 99% CI: [{ci_bounds[99][0]}, {ci_bounds[99][1]}]\"\n","\n","            # Append results with bootstrapped confidence intervals\n","            results.append(WilcoxonTestResults(col, stat, p_value, None, df,\n","                                                        ci_bounds[95][0], ci_bounds[95][1],\n","                                                        ci_bounds[99][0], ci_bounds[99][1],\n","                                                        'wilcoxon', median_diff, margin_of_err95, margin_of_err99,\n","                                                        formatted_ci_95, formatted_ci_99\n","                                              ))\n","\n","        else:\n","            # Calculate the mean difference / point estimate for t-test\n","            mean_diff = np.mean(treatment_col - control_col)\n","\n","            # Paired t-test\n","            l = ttest_rel(treatment_col, control_col)\n","\n","            df = len(control_col) - 1\n","\n","            try:\n","                ci_95 = l.confidence_interval(confidence_level=0.95)\n","                ci_99 = l.confidence_interval(confidence_level=0.99)\n","            except AttributeError:\n","                print(f\"Column: {col} - 'confidence_interval' method is not available. Update SciPy or use an alternative method.\")\n","                continue\n","\n","            # Accumulate p-values for numerical tests\n","            numerical_p_values.append(l.pvalue)\n","\n","                    # margin of error calculation:\n","            margin_of_err95 = (ci_95.high - ci_95.low) / 2\n","            margin_of_err99 = (ci_99.high - ci_99.low) / 2\n","\n","            # point estimate [CI lower, CI upper]:\n","            formatted_ci_95 = f\"{mean_diff}, 95% CI: [{ci_95.low}, {ci_95.high}]\"\n","            formatted_ci_99 = f\"{mean_diff}, 99% CI: [{ci_99.low}, {ci_99.high}]\"\n","\n","\n","            # Append results with None for adj_p_value\n","            results.append(TestResults(col, l.statistic, l.pvalue, None, df, ci_95.low, ci_95.high, ci_99.low, ci_99.high, 'ttest', mean_diff, margin_of_err95, margin_of_err99,\n","                                      formatted_ci_95, formatted_ci_99\n","                                      ))\n","\n","    # Accumulate all p-values from both numerical and categorical tests\n","    all_p_values = numerical_p_values\n","\n","    # Apply Hochberg correction to the combined p-values\n","    adj_all_p_values = false_discovery_control(all_p_values, method='bh')\n","\n","    # Separate the adjusted p-values back into numerical and categorical\n","    adj_numerical_p_values = adj_all_p_values[:len(numerical_p_values)]\n","\n","    # Assign the adjusted p-values back to the results\n","    for i in range(len(results)):\n","        results[i] = results[i]._replace(adj_p_value=adj_numerical_p_values[i])\n","\n","    # Convert results into a DataFrame\n","    results_df = pd.DataFrame(results)\n","\n","    return results_df\n","\n","# Example usage for oa_inc_matched_df\n","non_norm_columns_oa_inc = oa_inc_matched_df.iloc[:,-110:].columns.tolist()\n","results_df_oa_inc = perform_statistical_tests(oa_inc_matched_df, 'oa_prog', -110, non_norm_columns_oa_inc)\n","\n","# Example usage for tkr_matched_df\n","non_norm_columns_tkr = tkr_matched_df.iloc[:,-110:].columns.tolist()\n","results_df_tkr = perform_statistical_tests(tkr_matched_df, 'tkr', -110, non_norm_columns_tkr)\n","\n","# Optionally, save results to Excel\n","# results_df_oa_inc.to_excel(\"publish_dataframes/OA_Inc_meanDifference_wilcoxon_hochbergCorrected_results.xlsx\", index=False)\n","# results_df_tkr.to_excel(\"publish_dataframes/TKR_meanDifference_wilcoxon_hochbergCorrected_results.xlsx\", index=False)\n"],"metadata":{"id":"e4DSbwugumDC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Output Statistics**:\n","\n","*results_df_oa_inc*: \"publish_dataframes/OA_Inc_meanDifference_wilcoxon_hochbergCorrected_results.xlsx\"\n","\n","*results_df_tkr*: \"publish_dataframes/TKR_meanDifference_wilcoxon_hochbergCorrected_results.xlsx\""],"metadata":{"id":"DegRvR_2mf6r"}},{"cell_type":"code","source":[],"metadata":{"id":"3zd91Bqozcw0"},"execution_count":null,"outputs":[]}]}